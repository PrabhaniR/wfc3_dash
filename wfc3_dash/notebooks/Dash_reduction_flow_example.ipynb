{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to showcase the basic functioning of the wfc3_dash module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything here can also be run under a single main function, explained at the end of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "* *astroquery.mast Observations* used to download IMA files from the MAST HST archive\n",
    "* *astropy.io import fits* used to open the files\n",
    "* *matplotlib.pyplot* used to plot the images\n",
    "* *numpy* used for some math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroquery.mast import Observations\n",
    "from astropy.io import fits \n",
    "from astropy.table import Table\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from drizzlepac import tweakreg\n",
    "from drizzlepac import astrodrizzle\n",
    "\n",
    "%matplotlib notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The wfc3_dash submodule of wfc3_tools is used to reduce the effects of the spacecraft drift for WFC3/IR images taken in DASH mode (i.e. under GYRO control, rather than under Fine-Guide-Sensor control) \n",
    "\n",
    "This notebook works on a single .flt file but can be easily adapted to work on all exposures within a DASH visit or even a DASH program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading some relevant data\n",
    "\n",
    "#### Get the table of observations associated to GO-14114 (PI van Dokkum, the first proposal to use the DASH mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsTable = Observations.query_criteria(proposal_id=['14114'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the full list of products associated to the table and restric the list to IMA files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "product_list = Observations.get_product_list(obsTable)\n",
    "BM = (product_list['productSubGroupDescription']  == 'IMA') \n",
    "product_list = product_list[BM]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display (part of) the IMA files list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "product_list.show_in_notebook(display_length=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick a single exposure file to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myID = product_list['obsID'][0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the IMA and FLT files for that exposure. The standard pipeline-FLT will be used for comparison with the detrended final product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download = Observations.download_products(myID,mrp_only=False,productSubGroupDescription=['IMA','FLT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the results of the download operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the files that were just downloaded locally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#have path be everything minus last 8 characters (ima.fits)\n",
    "localpathtofile = download['Local Path'][0][:-8]\n",
    "localpathtofile\n",
    "\n",
    "original_ima = fits.open(localpathtofile+'ima.fits')\n",
    "original_flt = fits.open(localpathtofile+'flt.fits')\n",
    "original_ima.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the individual reads of the IMA file\n",
    "Note: the individual 'SCI' extensions are stored in reverse order, with 'SCI', 1 corresponding to the last read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamp = original_ima[0].header['NSAMP']\n",
    "print('NSAMP',nsamp)\n",
    "fig,axarr = plt.subplots((nsamp+3)//4,4, figsize=(9,3*((nsamp+3)//4)))\n",
    "\n",
    "for i in range(1,4*((nsamp+3)//4)+1):\n",
    "\n",
    "    row = (i-1)//4\n",
    "    col = (i-1)%4\n",
    "    if (i <= nsamp+1):\n",
    "        immed = np.nanmedian(original_ima['SCI',i].data)\n",
    "        stdev = np.nanstd(original_ima['SCI',i].data)\n",
    "        axarr[row,col].imshow(original_ima['SCI',i].data,clim=[immed-.3*stdev,immed+.5*stdev],cmap='Greys',origin='lower')\n",
    "        axarr[row,col].set_title('SCI '+str(i))\n",
    "        axarr[row,col].set_xticks([]) \n",
    "        axarr[row,col].set_yticks([]) \n",
    "    else:\n",
    "        fig.delaxes(axarr[row,col])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the individual steps of the DASH pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell is inserted temporarely to allow for relative imports until the whole wfc3_dash submodule is properly packaged and installed within the wfc3_tools module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from reduce_dash import DashData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a DashData object using the path to the ima file we have downloaded above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDash = DashData(localpathtofile+'ima.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create diff files. \n",
    "\n",
    "A diff file contains the counts accumulated between two reads.  \n",
    "The diff files are written to disk in a directory named ./diff under the current working directory (cwd).  \n",
    "In creating diff files, the first difference, between the 1-st and 0-th read is ignored becuase of   \n",
    "its very short expsoure time of 2.9 seconds, resulting in a noisy image.\n",
    "\n",
    "In order to create a correct error extension, the split_ima() method calls the utils.get_flat() function.  \n",
    "Such function reads the name of the flat field used for calibrating the ima images from the ima file header.  \n",
    "If the flat file is not present locally in a directory named ./iref under the cwd, get_flat() will download   \n",
    "the flat field file from the CRDS database https://hst-crds.stsci.edu/unchecked_get/references/hst/ \n",
    "and place it in ./iref ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDash.split_ima()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the diff files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndiff = len(myDash.diff_files_list)\n",
    "print('Number of diff files',ndiff)\n",
    "fig,axarr = plt.subplots((ndiff+3)//4,4, figsize=(9,3*((ndiff+3)//4)))\n",
    "\n",
    "for i in range(4*((ndiff+3)//4)):\n",
    "\n",
    "    row = (i)//4\n",
    "    col = (i)%4\n",
    "    if (i < ndiff):\n",
    "        diff_i = fits.open(myDash.diff_files_list[i]+'_diff.fits')\n",
    "        immed = np.nanmedian(diff_i['SCI'].data)\n",
    "        stdev = np.nanstd(diff_i['SCI'].data)\n",
    "        axarr[row,col].imshow(diff_i['SCI'].data,clim=[immed-.3*stdev,immed+.5*stdev],cmap='Greys',origin='lower')\n",
    "        axarr[row,col].set_title('Diff:'+str(i+1))\n",
    "        axarr[row,col].set_xticks([]) \n",
    "        axarr[row,col].set_yticks([]) \n",
    "    else:\n",
    "        fig.delaxes(axarr[row,col])\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create an association file\n",
    "\n",
    "This file mimics a typical association file for dithered exposures, that is used by astrodrizzle   \n",
    "to align and stack multiple exposures taken at the same sky position with small dithers.  \n",
    "We exploit the fact that a WFC3/IR exposure taken under gyro control can be effectively split into   \n",
    "individual pseudo-exposures (the diff images).  \n",
    "Astrodrizzle can treat such pseudo-expsoures as individual dithers, and comnbine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDash.make_pointing_asn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the content of the asn file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_filename = 'diff/{}_asn.fits'.format(myDash.root)\n",
    "asn_table = Table(fits.getdata(asn_filename, ext=1))\n",
    "asn_table.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Subtract Background from original FLT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is necessary in order to create a drizzled science array and a segmentation map to help subtract the background from the new FLT's. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDash.subtract_background_flt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Subtract Background from *new* FLT's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, subtract background from the individual reads taken from the original IMA file using the DRZ and SEG imaged produced in the background subtraction of the original FLT. <br>\n",
    "By default, this function will subtract the background and write it to the header. Setting parameter subtract to False will not subtract the background and only write it to the header. <br>\n",
    "Set parameter reset_stars_dq to True to reset cosmic rays within objects to 0 (because the centers of the stars are flagged)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDash.subtract_background_reads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Fix Cosmic Rays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDash.fix_cosmic_rays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7a. Align reads to catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine coordinates and search area from the WCS's of your images (procedure taken from Gaia_alignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from astropy import units as u\n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "from astropy.visualization import wcsaxes\n",
    "from astropy.coordinates.sky_coordinate import SkyCoord\n",
    "\n",
    "from matplotlib.patches import Polygon\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#use coordinates of original exposure\n",
    "def get_footprints(im_name):\n",
    "    \"\"\"Calculates positions of the corners of the science extensions of some image 'im_name' in sky space\"\"\"\n",
    "    footprints = []\n",
    "    hdu = fits.open(im_name)\n",
    "    \n",
    "    flt_flag = 'flt.fits' in im_name or 'flc.fits' in im_name\n",
    "    \n",
    "    # Loop ensures that each science extension in a file is accounted for. \n",
    "    for ext in hdu:\n",
    "        if 'SCI' in ext.name:\n",
    "            hdr = ext.header\n",
    "            wcs = WCS(hdr, hdu)\n",
    "            footprint = wcs.calc_footprint(hdr, undistort=flt_flag)\n",
    "            footprints.append(footprint)\n",
    "    \n",
    "    hdu.close()\n",
    "    return footprints\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "def bounds(footprint_list):\n",
    "    \"\"\"Calculate RA/Dec bounding box properties from multiple RA/Dec points\"\"\"\n",
    "    \n",
    "    # flatten list of extensions into numpy array of all corner positions\n",
    "    merged = [ext for image in footprint_list for ext in image]\n",
    "    merged = np.vstack(merged)\n",
    "    ras, decs = merged.T\n",
    "    \n",
    "    # Compute width/height\n",
    "    delta_ra = (max(ras)-min(ras))\n",
    "    delta_dec = max(decs)-min(decs)\n",
    "\n",
    "    # Compute midpoints\n",
    "    ra_midpt = (max(ras)+min(ras))/2.\n",
    "    dec_midpt = (max(decs)+min(decs))/2.\n",
    "    \n",
    "\n",
    "    return ra_midpt, dec_midpt, delta_ra, delta_dec\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            \n",
    "images = glob.glob(localpathtofile+'flt.fits')\n",
    "footprint_list = list(map(get_footprints, images))\n",
    "\n",
    "# # If that's slow, here's a version that runs it in parallel:\n",
    "# from multiprocessing import Pool\n",
    "# p = Pool(8)\n",
    "# footprint_list = list(p.map(get_footprints, images))\n",
    "# p.close()\n",
    "# p.join()\n",
    "\n",
    "ra_midpt, dec_midpt, delta_ra, delta_dec = bounds(footprint_list)\n",
    "\n",
    "coord = SkyCoord(ra=ra_midpt, dec=dec_midpt, unit=u.deg)\n",
    "print(coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying from Gaia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.units import Quantity\n",
    "from astroquery.gaia import Gaia\n",
    "\n",
    "width = Quantity(delta_ra, u.deg)\n",
    "height = Quantity(delta_dec, u.deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the query!\n",
    "r = Gaia.query_object_async(coordinate=coord, width=width, height=height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the table\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ras = r['ra']\n",
    "decs = r['dec']\n",
    "mags = r['phot_g_mean_mag']\n",
    "ra_error = r['ra_error']\n",
    "dec_error = r['dec_error']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aligning Data to Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table\n",
    "\n",
    "tbl = Table([ras, decs]) # Make a temporary table of just the positions\n",
    "tbl.write('gaia.cat', format='ascii.fast_commented_header') # Save the table to a file.  The format argument ensures\n",
    "                                                            # the first line will be commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 10.\n",
    "\n",
    "def get_error_mask(catalog, max_error):\n",
    "    \"\"\"Returns a mask for rows in catalog where RA and Dec error are less than max_error\"\"\"\n",
    "    ra_mask = catalog['ra_error']< max_error\n",
    "    dec_mask = catalog['dec_error'] < max_error\n",
    "    mask = ra_mask & dec_mask\n",
    "#     print('Cutting sources with error higher than {}'.format(max_error))\n",
    "#     print('Number of sources befor filtering: {}\\nAfter filtering: {}\\n'.format(len(mask),sum(mask)))\n",
    "    return mask\n",
    "\n",
    "mask = get_error_mask(r, thresh)\n",
    "\n",
    "tbl_filtered = Table([ras[mask], decs[mask]]) \n",
    "tbl.write('gaia_filtered_{}_mas.cat'.format(thresh), format='ascii.fast_commented_header')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls *cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Align images. <br>\n",
    "Will align to catalog by default. Parameters ref_catalog and ref_image denote the reference catalog and reference image, respectively. <br>\n",
    "To not align to catalog, set parameter align_method to ... <br>\n",
    "To not subtract background, set parameter subtract_background to False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[WIP]: Try to fix align function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from astropy.io.fits import getdata\n",
    "\n",
    "\n",
    "# asn_filename = 'diff/{}_asn.fits'.format(myDash.root)\n",
    "\n",
    "# asn_list=[]\n",
    "# for index, file in enumerate(myDash.diff_files_list):\n",
    "#     asn_list += [file +'_diff.fits']\n",
    "    \n",
    "# #asn_list.append(myDash.root)\n",
    "\n",
    "# # Create Primary HDU:\n",
    "# hdr = fits.Header()\n",
    "# hdr['FILENAME'] = \"'\" + asn_filename + \"'\"\n",
    "# hdr['FILETYPE'] = 'ASN_TABLE'\n",
    "# hdr['ASN_ID'] = \"'\" + myDash.root + \"'\"\n",
    "# hdr['ASN_TABLE'] = \"'\" + asn_filename + \"'\"\n",
    "# hdr['COMMENT'] = \"This association table is for the read differences for the IMA.\"\n",
    "# primary_hdu = fits.PrimaryHDU(header=hdr)\n",
    "\n",
    "# # Create the information in the asn file\n",
    "# num_mem = len(asn_list)\n",
    "\n",
    "# asn_mem_names = np.array(asn_list)\n",
    "# asn_mem_types =  np.full(num_mem,'EXP-DTH',dtype=np.chararray)\n",
    "# asn_mem_types[-1] = 'PROD-DTH'\n",
    "# asn_mem_prsnt = np.ones(num_mem, dtype=np.bool_)\n",
    "# asn_mem_prsnt[-1] = 0\n",
    "\n",
    "# hdu_data = fits.BinTableHDU().from_columns([fits.Column(name='MEMNAME', format='17A', array=asn_mem_names), \n",
    "#             fits.Column(name='MEMTYPE', format='14A', array=asn_mem_types), \n",
    "#             fits.Column(name='MEMPRSNT', format='L', array=asn_mem_prsnt)])\n",
    "\n",
    "# # Create the final asn file\n",
    "# hdu = fits.HDUList([primary_hdu, hdu_data])\n",
    "\n",
    "# if 'EXTEND' not in hdu[0].header.keys():\n",
    "#     hdu[0].header.update('EXTEND', True, after='NAXIS')\n",
    "        \n",
    "# hdu.writeto(asn_filename, overwrite=True)\n",
    "\n",
    "# data = getdata(asn_filename, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myDash.align(align_method='CATALOG', ref_catalog = 'gaia.cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot aligned science images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sci = fits.getdata('final_drz_sci.fits')\n",
    "\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "plt.imshow(sci, vmin=-0.05, vmax=0.4, cmap='Greys_r', origin='lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b. Align reads to each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative option to aligning reads to catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDash.make_read_catalog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using main function to reduce data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
